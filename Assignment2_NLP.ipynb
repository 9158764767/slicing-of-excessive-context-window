{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The implementation here structured to perform a series of tasks primarily focused on processing text for use with a language model, likely in the context of handling large inputs for chat models like ChatGPT. Here’s a breakdown of what each major part of the code is doing:\n",
        "\n",
        "## Importing Libraries and Preparing the Environment:\n",
        "Imports libraries like nltk for natural language processing, BeautifulSoup for HTML parsing, and sklearn for text vectorization and similarity calculations.\n",
        "Downloads necessary resources from NLTK, such as tokenizers and stopwords.\n",
        "\n",
        "## Reading and Preprocessing Input Text:\n",
        "Reads text from a file named input.txt.\n",
        "Cleans the text using BeautifulSoup to remove HTML content.\n",
        "Processes the text by tokenizing, converting to lowercase, removing non-alphanumeric characters, and eliminating stopwords. It also performs lemmatization to reduce words to their base or dictionary form.\n",
        "\n",
        "## Context Window Slicing Algorithm:\n",
        "Defines a function to handle large texts by slicing them into smaller segments or \"slices\" that fit within a specified context window size (e.g., 128 MB).\n",
        "Implements a method to ensure these slices are distinct enough from each other using a cosine similarity threshold. This is achieved by converting text slices into TF-IDF vectors and calculating the cosine similarity between them.\n",
        "\n",
        "## Saving Slices to a File:\n",
        "Outputs the generated text slices to a file named slices_output.txt, which presumably would be used for further processing or directly interfacing with a model.\n",
        "Interacting with an AI Model via the Replicate API:\n",
        "Installs and imports the replicate library to interact with AI models hosted on the Replicate platform.\n",
        "Sets up API tokens and specifies a model (e.g., meta/llama-2-70b-chat) for generating responses based on the sliced input.\n",
        "\n",
        "## Model Interaction and Output Handling:\n",
        "Reads the sliced text from slices_output.txt and sends it to the specified AI model.\n",
        "Initializes a conversation with the model using the sliced input, and handles responses either through streaming or direct prediction methods.\n",
        "Collects and prints responses, which appears to be aimed at testing or demonstrating the model interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7OQpsM4djqv",
        "outputId": "edd508ef-2cd0-487e-8d48-20b84b692dd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read input text from a file\n",
        "with open('input.txt', 'r', encoding='utf-8') as file:\n",
        "    input_text = file.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Following code comprises two main parts: a text preprocessing function and a context window slicing algorithm. Here's what each part does:\n",
        "\n",
        "# 1. Preprocessing Function (`preprocess_text`)\n",
        "This function is designed to prepare text data for further processing or analysis by cleaning and normalizing it:\n",
        " HTML Tag Removal: Utilizes `BeautifulSoup` to parse the given text and remove any HTML tags. The clean text is then extracted with spaces as separators to ensure that no words are concatenated together after tag removal.\n",
        " Removal of Non-alphabetic Characters: Uses a regular expression (via `re.sub`) to eliminate any characters that are not letters or whitespace, simplifying the text.\n",
        "Tokenization and Case Normalization: Converts the cleaned text to lowercase and splits it into individual words (tokens) using NLTK's `word_tokenize`.\n",
        " Alphanumeric Filtering: Filters out any tokens that are not strictly alphanumeric, removing any remnants like standalone numbers or punctuation marks that might have been missed.\n",
        " Stopword Removal: Eliminates common English stopwords (using a predefined list from NLTK) to reduce noise and focus on more meaningful words.\n",
        " Lemmatization: Applies lemmatization to the remaining tokens to reduce them to their base or dictionary form (lemmas), facilitating uniformity and reducing the complexity of subsequent processing.\n",
        "\n",
        "The function returns the processed text as a single string, where tokens are joined back together with spaces.\n",
        "\n",
        "# 2. Context Window Slicing Algorithm (`generate_slices`)\n",
        "This function is designed to manage large inputs that might exceed the processing capabilities of certain systems (like NLP models with a maximum context size):\n",
        "Context Size Calculation: It sets a context window size limit (128 MB by default, converted to bytes).\n",
        "Text Splitting into Slices: Splits the preprocessed text into smaller \"slices\" that fit within this byte size limit. This is done by checking the byte size of concatenated words, ensuring each slice does not exceed the maximum context window size.\n",
        " Cosine Similarity for Differentiation: Enhances the distinctiveness of consecutive slices using TF-IDF vectorization and cosine similarity calculation:\n",
        "   Converts the text of the last slice and the current candidate slice into TF-IDF vectors.\n",
        "   Calculates the cosine similarity between these vectors.\n",
        "   If the similarity is below a threshold (0.2 in this case), it implies that the slices are sufficiently different, and the candidate slice is added to the list of final slices. This step ensures that the slices are unique enough, preventing redundancy.\n",
        "\n",
        "The algorithm outputs an array of text slices that are optimized for size and distinctiveness, suitable for processing in systems with strict input size limits it prints out the generated slices, which can be directly used for feeding into systems like large language models (e.g., GPT models) that have constraints on the maximum input length they can handle efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6C0AhtLj-rz",
        "outputId": "0757628d-1af4-4912-f38c-4ad021a560f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['exploration space stand one humanity greatest achievement moon landing marked pinnacle space race current endeavor aim even higher targeting mar beyond advancement rocket technology satellite system unmanned spacecraft opened new frontier scientific discovery potential human settlement researcher engineer around world collaborate overcome physical technological challenge interstellar travel cosmic radiation life support system sustainable food production space realm economics st century witnessed seismic shift towards globalization digital transaction rise cryptocurrencies blockchain technology challenge traditional banking system fiat currency proposing new era decentralized finance economist debate implication digital currency global financial stability autonomy national economy meanwhile international trade agreement tariff continue shape economic landscape country influencing job market industry growth consumer price cultural tapestry world rich diverse community contributing unique blend tradition language art cultural preservation become critical challenge age globalization homogenization threatens erase distinct cultural identity effort protect linguistic diversity traditional craft indigenous ritual pivotal maintaining cultural heritage community worldwide festival museum educational program play significant role celebrating educating cultural diversity enriches global society environmental conservation another critical issue facing planet effect climate change rising sea level increased frequency extreme weather event loss biodiversity demand urgent action conservation strategy include protecting natural habitat restoring ecosystem promoting biodiversity sustainable practice agriculture forestry fishing essential preserve earth resource future generation international cooperation necessary address environmental challenge cross national border air pollution ocean plastic wildlife trafficking technological innovation continues revolutionize daily life development artificial intelligence machine learning robotics provides tool enhance productivity solve complex problem undertake task would dangerous human technology application medicine manufacturing transportation among field however also raise ethical question job displacement privacy potential autonomous system make decision previously made human healthcare advancement significantly improved life expectancy quality life breakthrough genetics crispr genome mapping offer potential treat even eradicate genetic disorder telehealth provides accessible healthcare remote area making medical advice monitoring available easily visit medical facility public health strategy medical research continue evolve especially response global health crisis pandemic conclusion rapid pace technological scientific advancement present new opportunity also pose challenge require careful consideration responsible management society progress decision made today shape future generation come making imperative balance innovation ethical consideration sustainability']\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text(separator=\" \")  # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-alphabetic characters\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lower case\n",
        "    tokens = [token for token in tokens if token.isalnum()]  # Alphanumeric filter\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]  # Stopword removal\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatization\n",
        "    return \" \".join(lemmatized)\n",
        "\n",
        "processed_input = preprocess_text(input_text)\n",
        "\n",
        "# Context Window Slicing Algorithm\n",
        "def generate_slices(input_text, context_window_size=128):\n",
        "    context_window_bytes = context_window_size * 1024  # Adjust byte size \n",
        "    words = processed_input.split()\n",
        "    slices = []\n",
        "    current_slice = \"\"\n",
        "    for word in words:\n",
        "        if len(current_slice.encode('utf-8')) + len(word.encode('utf-8')) <= context_window_bytes:\n",
        "            current_slice += \" \" + word\n",
        "        else:\n",
        "            slices.append(current_slice.strip())\n",
        "            current_slice = word\n",
        "    if current_slice:\n",
        "        slices.append(current_slice.strip())\n",
        "\n",
        "    # Enhance slice differentiation using cosine similarity\n",
        "    final_slices = [slices[0]]\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    for i in range(1, len(slices)):\n",
        "        tfidf_matrix = vectorizer.fit_transform([final_slices[-1], slices[i]])\n",
        "        cosine_dist = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "        if cosine_dist < 0.2:  # Threshold for differentiation\n",
        "            final_slices.append(slices[i])\n",
        "\n",
        "    return final_slices\n",
        "\n",
        "slices = generate_slices(input_text)\n",
        "print(slices)  # Print the generated slices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This  code is designed to save the slices of text generated by the context window slicing algorithm into a file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VpfKWEfyj-z6"
      },
      "outputs": [],
      "source": [
        "# Save slices to a file\n",
        "with open('slices_output.txt', 'w', encoding='utf-8') as output_file:\n",
        "    for i, slice_text in enumerate(slices):\n",
        "        output_file.write(f\"Slice {i + 1}: {slice_text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Library:\n",
        "import replicate: Imports the replicate library into the Python environment. This library provides functions and classes to easily interact with models available on Replicate.\n",
        "# Authentication and Model Specification:\n",
        "REPLICATE_API_TOKEN = \"r8_aiebuYTaLBZIzYv8whiaYlsKcqLH43p2nexiP\": Defines a variable to store the API token. This token is used for authenticating the user's session when making API requests to Replicate. It ensures that the user has the rights to access the models and other resources on the platform.\n",
        "client = replicate.Client(api_token=REPLICATE_API_TOKEN): Creates an instance of the Client class from the replicate library, initializing it with the API token. This client object is used to interact with the API.\n",
        "model_name = \"meta/llama-2-70b-chat\": Sets the name of the model that the script will interact with. In this case, it’s specifying the \"llama-2-70b-chat\" model hosted by Meta. This model is likely a large language model designed for understanding and generating human-like text, useful for tasks like chatting, answering questions, or any other NLP-based task.\n",
        "# Purpose:\n",
        "This setup is prepared to facilitate the interaction with a specific AI model via the Replicate API, using the provided API token for authentication. The user can now proceed to make calls to the model using the client instance, passing data to the model and receiving responses. This can be used in applications where AI-generated text is needed, such as chatbots, automated responses, content generation, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q9wtGvsj-39",
        "outputId": "f244e13a-918e-4f45-b751-0208d7adfdae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting replicate\n",
            "  Downloading replicate-0.25.2-py3-none-any.whl (39 kB)\n",
            "Collecting httpx<1,>=0.21.0 (from replicate)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (24.0)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.10/dist-packages (from replicate) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.11.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.21.0->replicate)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.21.0->replicate)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (2.18.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.1)\n",
            "Installing collected packages: h11, httpcore, httpx, replicate\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 replicate-0.25.2\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed urllib3-2.2.1\n"
          ]
        }
      ],
      "source": [
        "#! pip install replicate\n",
        "#! pip install --upgrade requests urllib3\n",
        "\n",
        "\n",
        "import replicate\n",
        "# Authenticate with Replicate API and define the model\n",
        "REPLICATE_API_TOKEN = \"r8_aiebuYTaLBZIzYv8whiaYlsKcqLH43p2nexiP\" #API generated from https://replicate.com/account/api-tokens # alternative token : r8_JILYxSzs7MDHpGHdDPP2KJ1aP9fQa8f2PnlvV\n",
        "client = replicate.Client(api_token=REPLICATE_API_TOKEN)\n",
        "model_name = \"meta/llama-2-70b-chat\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reading the Sliced Text from a File:\n",
        "with open('slices_output.txt', 'r', encoding='utf-8') as input_file:: This line opens the file slices_output.txt for reading in text mode with UTF-8 encoding. The with statement ensures that the file is properly closed after its suite finishes.\n",
        "slice_text = input_file.read(): Reads all the content from the file into the variable slice_text. This text is presumably the output from a previous operation where text was sliced into smaller segments suitable for processing.\n",
        "\n",
        "# Initializing the Conversation with the Model:\n",
        "print(\"Initializing the conversation with the model...\"): Simply prints a message indicating that the conversation with the model is about to start.\n",
        "initial_response = client.stream(...): Uses the stream method of the client object to begin streaming input to the model. The input is formatted with a prompt that includes the read text followed by a placeholder for user input. This is likely used for models designed to handle conversational contexts or ongoing inputs.\n",
        "\n",
        "# Alternative Method to Initialize the Conversation:\n",
        "Another print statement with the same message; this could be an oversight or meant for clarity in case of different stages of interaction.\n",
        "The try-except block attempts to use the predict method (or a similar method suitable for the task):\n",
        "response = client.predict(...): Tries to send the text to the model using a method that immediately returns a prediction rather than streaming the input. This method is suitable for scenarios where a direct response is expected without the need for an ongoing interaction.\n",
        "print(response): Outputs the response from the model to the console.\n",
        "If the predict method doesn't exist or some other attribute-related error occurs, it catches the AttributeError and prints a failure message.\n",
        "\n",
        "# Handling the Streamed Response:\n",
        "user_input = \"\": Initializes an empty string to collect outputs from the model.\n",
        "for event in initial_response:: Iterates over each event in the response stream.\n",
        "user_input += str(event): Appends the string representation of each event to user_input. This is likely collecting all parts of the model's response to form a complete answer or interaction sequence.\n",
        "print(event, end=\"\"): Prints each event to the console without adding a newline after each event, effectively streaming the output live as it is received.\n",
        "# Purpose:\n",
        "This script is set up to interact with an AI model, potentially in a conversational manner, using text previously prepared and sliced into manageable parts. It demonstrates both a continuous interaction mode (streaming) and a one-off prediction mode, accommodating different operational needs depending on the model's capabilities or the desired interaction pattern.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0BLIeyBj-7J",
        "outputId": "65e57d1c-29ac-4c4f-9ebd-2436bd480bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing the conversation with the model...\n",
            "Initializing the conversation with the model...\n",
            "Failed to interact with the model: 'Client' object has no attribute 'predict'\n",
            " Sure, I can help you with that. Here's a summary of the input you provided:\n",
            "\n",
            "The moon landing was a significant achievement for humanity, representing the pinnacle of the space race and opening up new frontiers for scientific discovery and potential human settlement. However, there are still physical and technological challenges to overcome, such as cosmic radiation, life support systems, and sustainable food production. The rise of cryptocurrencies and blockchain technology has challenged traditional banking and financial systems, with economists debating the implications for global financial stability and national autonomy.\n"
          ]
        }
      ],
      "source": [
        "# Read the sliced text\n",
        "with open('slices_output.txt', 'r', encoding='utf-8') as input_file:\n",
        "    slice_text = input_file.read()\n",
        "\n",
        "# Initialize the conversation with the model using the sliced text\n",
        "print(\"Initializing the conversation with the model...\")\n",
        "initial_response = client.stream(\n",
        "    model_name,\n",
        "    input={\n",
        "        \"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input: \"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Initialize the conversation with the model using the sliced text\n",
        "print(\"Initializing the conversation with the model...\")\n",
        "try:\n",
        "    # Using the predict method if available, or adjust based on actual available method\n",
        "    response = client.predict(model_name, input={\"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input: \"})\n",
        "    print(response)\n",
        "except AttributeError as e:\n",
        "    print(\"Failed to interact with the model:\", e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Retrieve and store the initial model output\n",
        "user_input = \"\"\n",
        "for event in initial_response:\n",
        "    user_input += str(event)  # Collect initial output to continue the conversation\n",
        "    print(event, end=\"\")  # Display the model's initial output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeoV5nyYkb1v",
        "outputId": "a49ea69f-0c75-405d-ae0d-2a249882ce69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You: what is moon\n",
            "\n",
            "Asking your question to the model...\n",
            " The moon is the natural satellite of Earth, orbiting our planet at an average distance of about 239,000 miles (384,000 kilometers). It is the fifth-largest satellite in the solar system and the largest satellite relative to the size of its planet. The moon has a diameter of about 2,159 miles (3,475 kilometers), which is about one-quarter the size of Earth.\n",
            "\n",
            "The moon is a rocky, airless body with no atmosphere, and its surface is characterized by mountains, craters, and"
          ]
        }
      ],
      "source": [
        "# Ask the user for their question\n",
        "user_question = input(\"\\nYou: \")\n",
        "\n",
        "# Continue the conversation based on the user's question\n",
        "print(\"\\nAsking your question to the model...\")\n",
        "response = client.stream(\n",
        "    model_name,\n",
        "    input={\n",
        "        \"prompt\": f\"{user_input}\\n\\nUser Question: {user_question}\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Print the model's response to the user's question\n",
        "for event in response:\n",
        "    print(event, end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2XrrTwokcLI",
        "outputId": "7d3e4f3e-7ca3-43df-e486-4676dcf958e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing the conversation with the model...\n",
            "Response: [' Thank', ' you', ' for', ' the', ' input', '.', ' It', ' appears', ' to', ' be', ' a', ' collection', ' of', ' various', ' topics', ' and', ' issues', ' that', ' are', ' currently', ' being', ' discussed', ' in', ' the', ' world', '.', ' It', \"'\", 's', ' cru', 'cial', ' to', ' approach', ' these', ' subjects', ' with', ' care', ' and', ' consideration', ',', ' taking', ' into', ' account', ' the', ' eth', 'ical', ',', ' soci', 'etal', ',', ' and', ' environmental', ' effects', ' they', ' may', ' have', '.', '\\n', '\\n', 'In', ' terms', ' of', ' techn', 'ological', ' development', ',', ' it', ' is', ' cru', 'cial', ' to', ' invest', ' in', ' cutting', '-', 'edge', ' techn', 'ologies', ' including', ' artificial', ' intelligence', ',', ' robot', 'ics', ',', ' and', ' gen', 'et', 'ics', '.', ' These', ' techn', 'ologies', ' have', ' the', ' potential', ' to', ' significantly', ' enh', 'ance', ' product', 'ivity', ' across', ' a', ' variety', ' of', ' indust', 'ries', ',', ' including', ' health', 'care', ',', ' transport', 'ation', ',', ' and', ' manufact', 'uring', '.', ' But', ' it', \"'\", 's', ' cru', 'cial', ' to']\n",
            " Thank you for the input. It appears to be a collection of various topics and issues that are currently being discussed in the world. It's crucial to approach these subjects with care and consideration, taking into account the ethical, societal, and environmental effects they may have.\n",
            "\n",
            "In terms of technological development, it is crucial to invest in cutting-edge technologies including artificial intelligence, robotics, and genetics. These technologies have the potential to significantly enhance productivity across a variety of industries, including healthcare, transportation, and manufacturing. But it's crucial to\n",
            "You: what is moon\n",
            "\n",
            "Asking your question to the model...\n",
            " The moon is the natural satellite of Earth, orbiting our planet at an average distance of about 239,000 miles (384,000 kilometers). It is the fifth-largest satellite in the solar system and the largest satellite relative to the size of its planet. The moon has a diameter of about 2,159 miles (3,475 kilometers), which is about one-quarter the size of Earth.\n",
            "\n",
            "The moon is thought to have formed about 4.5 billion years ago, not long after the formation of the Earth. One theory"
          ]
        }
      ],
      "source": [
        "import replicate\n",
        "\n",
        "# Authenticate with Replicate API and define the model\n",
        "REPLICATE_API_TOKEN = \"r8_aiebuYTaLBZIzYv8whiaYlsKcqLH43p2nexiP\" # alternative token : r8_JILYxSzs7MDHpGHdDPP2KJ1aP9fQa8f2PnlvV\n",
        "client = replicate.Client(api_token=REPLICATE_API_TOKEN)\n",
        "model_name = \"meta/llama-2-70b-chat\"\n",
        "\n",
        "# Read the sliced text\n",
        "with open('slices_output.txt', 'r', encoding='utf-8') as input_file:\n",
        "    slice_text = input_file.read()\n",
        "\n",
        "# Assuming we found that the correct method is `run` or similar\n",
        "print(\"Initializing the conversation with the model...\")\n",
        "try:\n",
        "    initial_response = client.run(model_name, {\"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input: \"})\n",
        "    print(\"Response:\", initial_response)\n",
        "except AttributeError as e:\n",
        "    print(\"Failed to interact with the model:\", e)\n",
        "\n",
        "# Assuming initial_response is iterable if correct method is used\n",
        "user_input = \"\"\n",
        "for event in initial_response:\n",
        "    user_input += str(event)  # Collect initial output to continue the conversation\n",
        "    print(event, end=\"\")  # Display the model's initial output\n",
        "\n",
        "# Ask the user for their question\n",
        "user_question = input(\"\\nYou: \")\n",
        "\n",
        "# Continue the conversation based on the user's question\n",
        "print(\"\\nAsking your question to the model...\")\n",
        "try:\n",
        "    response = client.run(model_name, {\"prompt\": f\"{user_input}\\n\\nUser Question: {user_question}\"})\n",
        "    for item in response:\n",
        "        print(item, end=\"\")  # Assuming response is iterable\n",
        "except AttributeError as e:\n",
        "    print(\"Failed to interact with the model:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "                                            # Dynamic implementation for multiple level convercession with model.\n",
        "# Reading Sliced Text: \n",
        "The script reads from slices_output.txt, which contains pre-processed text suitable for model consumption. This text is read in its entirety into the slice_text variable.\n",
        "\n",
        "# Initial Conversation Initialization\n",
        "The script prints a message to indicate that the conversation with the model is being initialized.\n",
        "It attempts to start the conversation by sending a prompt to the model that includes the sliced text and an additional placeholder for \"User Input\".\n",
        "client.run(...): This method sends a complete input to the model and waits for a full response (not streaming).\n",
        "It iterates through the model's initial response and prints it. This part of the interaction is enclosed in a try-except block to handle cases where an attribute or method might not be supported by the client or model.\n",
        "\n",
        "# Interactive Loop for Continuous Conversation\n",
        "A while True: loop is used to continuously interact with the model based on user input.\n",
        "user_question = input(\"\\nYou: \"): Prompts the user to enter their question or response.\n",
        "The script sends this new user question to the model, appending it to the ongoing context (user_input), and prints the model's response. This assumes that user_input was previously defined and stores the ongoing conversation context, although it isn't explicitly shown in the initial part of the script you provided.\n",
        "Responses from the model are printed out sequentially.\n",
        "\n",
        "# Termination Condition\n",
        "After each iteration (exchange with the model), the user is asked if they want to continue the conversation.\n",
        "If the user responds with anything other than \"yes,\" the script prints a termination message and breaks out of the loop, ending the program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIsdrrhKpC5d",
        "outputId": "7fcad651-97c9-434a-f7b4-0eca378d4f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing the conversation with the model...\n",
            "Model Response:\n",
            " Thank you for the input. It seems like you've provided a comprehensive overview of various topics, including space exploration, globalization, cultural diversity, environmental conservation, technological innovation, and healthcare advancements. It's impressive to see how you've connected these different areas and highlighted their significance in shaping our world.\n",
            "\n",
            "However, I must respectfully point out that your input doesn't contain a specific question or request for information. I'm here to assist you with any inquiries or concerns you might have, but I cannot provide a response without a clear\n",
            "You: what is moon\n",
            "\n",
            "Asking your question to the model...\n",
            "Model Response:\n",
            " The moon is the natural satellite of Earth, orbiting our planet at an average distance of about 239,000 miles (384,000 kilometers). It is the fifth-largest satellite in the solar system and the largest satellite relative to the size of its planet. The moon has a diameter of about 2,159 miles (3,475 kilometers), which is about one-quarter the size of Earth.\n",
            "\n",
            "The moon is thought to have formed about 4.5 billion years ago, not long after the formation of the Earth. One theory\n",
            "Do you want to continue the conversation? (yes/no): yes\n",
            "\n",
            "You: what is india\n",
            "\n",
            "Asking your question to the model...\n",
            "Model Response:\n",
            " India is a country located in South Asia. It is the seventh-largest country by area and the second-most populous country in the world with over 1.3 billion people. India is a federal republic governed under a parliamentary system and is divided into 29 states and 7 union territories. The capital of India is New Delhi. The official languages of India are Hindi and English.\n",
            "\n",
            "India has a diverse landscape, which ranges from the Himalayan mountains in the north to the Indian Ocean in the south. It is home to several major river systems, including the\n",
            "Do you want to continue the conversation? (yes/no): no\n",
            "Exiting the conversation.\n"
          ]
        }
      ],
      "source": [
        "import replicate\n",
        "\n",
        "# Authenticate with Replicate API and define the model\n",
        "REPLICATE_API_TOKEN = \"r8_aiebuYTaLBZIzYv8whiaYlsKcqLH43p2nexiP\" # alternative token : r8_JILYxSzs7MDHpGHdDPP2KJ1aP9fQa8f2PnlvV\n",
        "client = replicate.Client(api_token=REPLICATE_API_TOKEN)\n",
        "model_name = \"meta/llama-2-70b-chat\"\n",
        "\n",
        "# Read the sliced text\n",
        "with open('slices_output.txt', 'r', encoding='utf-8') as input_file:\n",
        "    slice_text = input_file.read()\n",
        "\n",
        "# Initialize the conversation with the model using the sliced text\n",
        "print(\"Initializing the conversation with the model...\")\n",
        "try:\n",
        "    initial_response = client.run(model_name, {\"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input: \"})\n",
        "    print(\"Model Response:\")\n",
        "    for event in initial_response:\n",
        "        print(event, end=\"\")  # Display the model's initial output\n",
        "except AttributeError as e:\n",
        "    print(\"Failed to interact with the model:\", e)\n",
        "\n",
        "# Loop for continuous conversation\n",
        "while True:\n",
        "    # Ask the user for their question\n",
        "    user_question = input(\"\\nYou: \")\n",
        "\n",
        "    # Continue the conversation based on the user's question\n",
        "    print(\"\\nAsking your question to the model...\")\n",
        "    try:\n",
        "        response = client.run(model_name, {\"prompt\": f\"{user_input}\\n\\nUser Question: {user_question}\"})\n",
        "        print(\"Model Response:\")\n",
        "        for item in response:\n",
        "            print(item, end=\"\")  # Display the model's response\n",
        "    except AttributeError as e:\n",
        "        print(\"Failed to interact with the model:\", e)\n",
        "\n",
        "    # Check if the user wants to continue the conversation\n",
        "    continue_conversation = input(\"\\nDo you want to continue the conversation? (yes/no): \")\n",
        "    if continue_conversation.lower() != 'yes':\n",
        "        print(\"Exiting the conversation.\")\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
